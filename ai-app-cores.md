# AI 应用的三大核心

## 前言

AI 技术快速发展，使得 AI 概念层出不穷，从普通的 chat 应用到 agent、RAG、Vibe Coding 等。这些概念的提出说明了用户在 AI 领域下的细分需求，但同时也让 AI 的上手入门学习成本变高。本文将结合自身学习，探讨 AI 应用的核心。

## AI 应用的核心

显然，AI 应用和传统应用的核心是增加了 AI 功能，故而 AI 功能的好坏直接影响到 AI 应用的好坏。

这里，我抛出我的观点：**AI 应用的质量取决于模型质量，上下文质量和提示词质量**。即

$$
V(AI\_APP) \propto V(M)V(C)V(P)
$$

> 其中 $V(AI\_APP)$ 表示 AI 应用的质量，$V(M)$ 表示模型质量，$V(C)$ 表示上下文质量，$V(P)$ 表示提示词质量。

## Chat
Chat 应用是 AI 应用中较为基础的一类应用，即 ChatGPT、Deepseek 等。这类应用没有较为复杂的功能形式，只有最基础问答功能。

Chat 应用的好坏主要取决于模型质量 $V(M)$ 和提示词质量 $V(P)$。

正是如此，开发这一类应用的厂商普遍在提升模型质量和模型理解上下文上下功夫。同时，用户在使用这一类应用时，也催生出大量提示词的使用技巧以提高回答质量，继续出现 Prompt Engineering 的概念。

Chat 应用目前的迭代早已不再局限于自然语言回答，很多应用已经开始支持代码、图片等多模态的回答形式。

## 工作流（Workflow）

用户发现 AI 的功能虽然很强大，但是面对复杂任务的时候，往往收效甚微，甚至文不对题。于是，尝试人为的将复杂任务拆分成多个子任务，将多个子任务逐步交给 AI 完成。这就是工作流的雏形。

在工作流中，用户可以通过定义一系列的步骤和规则，将复杂任务分解为简单的子任务，并通过 AI 助手逐步完成这些子任务。这种方式不仅提高了工作效率，也使得用户能够更好地控制任务的进度和质量。

通常，用户会将脚本和 AI 任务相结合以实现功能需求。用户可以通过编写脚本来处理数据、调用 API 等，然后将这些脚本与 AI 任务结合起来，实现更复杂的功能。

## Agent

但是，Workflow 仍然需要用户手动定义步骤和规则，面对复杂任务时，用户需要花费大量时间来设计和调整工作流。聪明的人开始考虑让 AI 拆分任务，并完成任务。

Agent 和 Workflow 的区别在于，Agent 能够自主地理解任务需求，并根据上下文和提示词自动生成工作流。用户只需提供任务目标，Agent 就能自动选择合适的模型和提示词，从而实现更高效的任务处理。

但是在 Agent 应用的开发过程中，开发者意识到在 Workflow 中用户可以通过脚本调用 API 来实现任务，但是 Agent 应用却无法直接调用 API。于是衍生出了 MCP。

## MCP

事实上，我认为 MCP（Model-Context-Prompt）仅仅是面向 AI 应用的 SDK 而已。

## Edit

事实上，从 Workflow 到 Agent 也不是一蹴而就的事情。我认为至少在这过程中，经历由 Edit 到 Agent 转变的阶段。

我认为，Edit 和 Agent 的区别在于访问的上下文不同。

- Edit 通常是针对单个任务的上下文进行编辑和修改
- Agent 则是针对整个任务流程的上下文进行处理

故而 Edit 可以访问且支持修改的上下文通常是用户指定的文件及对话上下文。而 Agent 通常可以访问整个任务流程的上下文，通常是工作空间等。

## Chat 模型

无论是 Chat、Agent 还是 Edit，其核心交互都是基于用户的自然语言输入（或代码），AI 负责理论用户意图并生成对应的回答（回答不一定是自然语言）。故而就目前来看，这三者统一为 Chat 模型的三种模式。

---

## RAG

在实践的过程中，用户或开发者发现，在某些特定的场景下，Chat 模型并不是那么好用。其上下文的大小限制了模型的理解能力，导致上下文质量不高，进而影响回答质量。

于是，为了在有限的上下文中尽量提供更多的信息，开发者们提出了 [RAG（Retrieval-Augmented Generation）](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)模型。

RAG 模型支持用户将大量的文档存储到知识库中，并通过检索相关文档来增强模型的上下文信息。这样，模型可以在回答问题时参考更多的信息，从而提高回答的质量。

那么如何让 AI 能够理解文档的内容，同时又能够理解用户的提问呢？

简单来说，RAG 模型将文档内容进行切片、索引、向量化（Embedding），以便于更好地检索和匹配相关信息。

### Embedding

这里简单说一下 Embedding 的概念。Embedding 是将文本转换为向量的过程。例如，将`小猫`这个词通过一系列的技术转成一个向量 `[1,2]`，而将`小狗`转换为 `[2,1]`。这样，模型就可以通过向量之间的距离来判断两个词的相似度。

事实上，在图像识别、语音识别等领域也有类似的应用。例如，将大量小猫的图片经过多层神经网络进行训练后所得到的值进行汇总统计过滤，得到某个区间，如（$10\pm2$）。后续当输入一张新的图片时，模型可以通过计算这张图片的值是否在这个区间内来判断这张图片是否是小猫。

---

我们讲上述切片、索引、向量化、存储到向量数据库等过程统称为索引（Indexing）。

RAG 模型根据用户的提问进行最相关文档的查询，这一过程称之为检索（Retrieval）。

在检索完成后，RAG 模型会讲用户的提问和检索到的文档内容做整合，形成新的上下文。这个过程称之为增强（Augmentation）。

最后，将增强后的上下文输入到 Chat 模型中进行回答生成，这个过程称之为生成（Generation）。

事实上，在目前的 RAG 模型中，有些模型会增加额外的步骤例如重排，上下文选择，微调的方式进一步优化输出的质量。

## Prompt Engineering

上文提到的[提示词工程（Prompt Engineering）](https://en.wikipedia.org/wiki/Prompt_engineering)为了适应 AI 在不同领域下的模型，同样也衍生出了新的概念和技术。

简单来说，由于在 AI 中，为了提升 $V(P)$，用户总结了一套提示词使用技巧并将其工程化，形成了 Prompt Engineering。

## Vibe Coding

在计算机领域，一个新的概念 Vibe Coding（氛围感编程）也被提出来。这是一种开发风格，并非某些特定的技术或工具。

Vibe Coding 强调的是在编程过程中，开发者与 AI 之间的互动和协作。即当你用 AI 进行编程的时候，你就是在 Vibe Coding 中。

## Context Engineering

当用户发现仅仅是 Prompt Engineering 还无法满足需求时，Context Engineering（上下文工程）应运而生。

事实上，这就是从提升 $V(P)$ 到提升 $V(C)$ 的改变。

人们发现越复杂的任务中，越需要对上下文做更加精细的管理。毕竟上下文是有限的，而上下文的质量会极大影响 AI 的输出。

例如，在复杂任务中，AI 应用会将上下文保存在某个地方（比如内存、数据库等），以便于后续的任务调用和上下文的复用。

同时，如果将上下文脱离对话进行存储，那么就出现记忆机制。这就使得可以实现跨对话上下文的复用和管理。并且，AI 应用也可以对上下文进行总结和提取，以生成针对当前用户的个性化上下文。

除此之外，还可以通过对上下文进行分析、处理、清洗的方式压缩上下文或者，根据不同的任务需求对上下文进行实现隔离等方式对上下文进行管理。

## 总结

就目前而言，对于 AI 从业者来说，在其理解 AI 底层逻辑的前提下，新的 AI 概念和技术提出是其对 AI 应用的细分需求和技术进步的体现。

而对受到 AI 冲击的其他行业来说，大量的 AI 概念提出则会让 AI 这一工具逐步高精尖化，从而降低 AI 的上手成本。

本门的目的是通过分析目前的 AI 应用，得出 AI 应用的核心是模型质量、上下文质量和提示词质量，从而使得 AI 应用开发者能够更好地理解和应用 AI 技术。

---
# references
- [Context Engineering](https://blog.langchain.com/context-engineering-for-agents/)